{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba916978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text       POS_         Tag_     Dep_           Lemma_    \n",
      "------------------------------------------------------------\n",
      "I          PRON         PRP      nsubj          I         \n",
      "love       VERB         VBP      ROOT           love      \n",
      "learning   VERB         VBG      xcomp          learn     \n",
      "Python     PROPN        NNP      dobj           Python    \n",
      "and        CCONJ        CC       cc             and       \n",
      "spaCy      NOUN         NNS      conj           spaCy     \n",
      "!          PUNCT        .        punct          !         \n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp('I love learning Python and spaCy!')\n",
    "\n",
    "print(f\"{'Text':<10} {'POS_':<12} {'Tag_':<8} {'Dep_':<14} {'Lemma_':<10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for token in doc:\n",
    "    # 打印每个token的文本、词性、详细标签、依存关系和词根原型\n",
    "    print(f\"{token.text:<10} {token.pos_:<12} {token.tag_:<8} {token.dep_:<14} {token.lemma_:<10}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7552730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Docker', 'Kubernetes', 'Java', 'plus', 'Go', 'Python']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    \n",
    "GENERIC_STOP_WORDS = {\n",
    "    \"experience\", \"skill\", \"ability\", \"knowledge\", \"understanding\",\n",
    "    \"responsibility\", \"role\", \"work\", \"developer\", \"engineer\", \"candidate\",\n",
    "    \"team\", \"year\", \"we\", \"you\", \"the\", \"a\", \"an\", \"and\", \"or\", \"in\", \"on\", \"at\"\n",
    "}\n",
    "TECH_WHITELIST = {\"java\", \"go\"}\n",
    "    \n",
    "def extract_tech_nouns(jd_text):\n",
    "  jdDoc = nlp(jd_text)\n",
    "  jdSet = set()\n",
    "  for jdToken in jdDoc:\n",
    "    if jdToken.pos_ not in jdToken.pos_ not in ['DET', 'ADP']:\n",
    "      continue\n",
    "    if jdToken.pos_ in ['NOUN', 'PROPN'] and jdToken.text.lower() not in GENERIC_STOP_WORDS and len(jdToken.text) > 2:\n",
    "      jdSet.add(jdToken.text)\n",
    "    elif jdToken.text.lower() in TECH_WHITELIST:\n",
    "      jdSet.add(jdToken.text)\n",
    "  return list(jdSet)\n",
    "\n",
    "extract_tech_nouns(\"We need a Python developer with experience in Docker, Kubernetes, and Go. Java experience is a plus.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b10f3e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分词结果： ['我', '喜欢', '用', 'Python', '编程', '和', 'Docker', '部署', '项目', '。']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# 加载官方中文模型\n",
    "nlp = spacy.load(\"zh_core_web_lg\")  # 根据你下载的模型调整名称\n",
    "\n",
    "# 测试分词\n",
    "test_text = \"我喜欢用Python编程和Docker部署项目。\"\n",
    "doc = nlp(test_text)\n",
    "print(\"分词结果：\", [token.text for token in doc])\n",
    "# 预期输出：['我', '喜欢', '用', 'Python', '编程', '和', 'Docker', '部署', '项目', '。']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739dda27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保护后: 需要熟悉Redis、MySQL和K8s技术，有AWS经验。\n",
      "术语映射: {}\n",
      "'阿里云': 技术词=True, 类别=云服务\n",
      "'数据库设计': 技术词=True, 类别=数据库\n",
      "'消息中间件': 技术词=True, 类别=中间件\n",
      "'普通经验': 技术词=False, 类别=None\n",
      "增强版提取结果: ['阿里云', 'mysql', 'is', 'k8s', 'AWS', 'Red', 'aws', '数据库', 'redis', 'MySQL', 'K8s']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "\n",
    "# 加载官方中文模型\n",
    "nlp = spacy.load(\"zh_core_web_lg\")  # 根据你下载的模型调整名称\n",
    "\n",
    "\n",
    "def protect_english_terms(text):\n",
    "    \"\"\"\n",
    "    保护英文术语，防止被错误分词\n",
    "    将连续的大写字母、数字、点号组成的术语用特殊标记保护起来\n",
    "    \"\"\"\n",
    "    # 匹配可能的技术术语：字母、数字、点、减号组成的连续字符串\n",
    "    pattern = r'\\b[A-Za-z][A-Za-z0-9.-]{1,}\\b'\n",
    "    protected_terms = {}\n",
    "    \n",
    "    def replace_match(match):\n",
    "        term = match.group()\n",
    "        placeholder = f\"__TERM_{len(protected_terms)}__\"\n",
    "        protected_terms[placeholder] = term\n",
    "        return placeholder\n",
    "    \n",
    "    # 先用占位符替换所有英文术语\n",
    "    protected_text = re.sub(pattern, replace_match, text)\n",
    "    return protected_text, protected_terms\n",
    "\n",
    "def restore_terms(text, protected_terms):\n",
    "    \"\"\"将保护起来的术语恢复回来\"\"\"\n",
    "    for placeholder, term in protected_terms.items():\n",
    "        text = text.replace(placeholder, term)\n",
    "    return text\n",
    "\n",
    "# 测试保护功能\n",
    "test_text = \"需要熟悉Redis、MySQL和K8s技术，有AWS经验。\"\n",
    "protected_text, term_dict = protect_english_terms(test_text)\n",
    "print(\"保护后:\", protected_text)\n",
    "print(\"术语映射:\", term_dict)\n",
    "\n",
    "# 使用更精确的模式库\n",
    "CHINESE_TECH_PATTERNS = {\n",
    "    '云服务': ['阿里云', '腾讯云', '华为云', 'AWS', 'Azure', '云原生', '云计算'],\n",
    "    '数据库': ['MySQL', 'Redis', 'MongoDB', 'PostgreSQL', 'Oracle', '数据库'],\n",
    "    '中间件': ['Kafka', 'RabbitMQ', 'RocketMQ', '中间件', '消息队列'],\n",
    "    '容器': ['Docker', 'Kubernetes', 'K8s', '容器', '容器化'],\n",
    "    '框架': ['Spring', 'Django', 'Flask', 'React', 'Vue', '微服务', '分布式']\n",
    "}\n",
    "\n",
    "\n",
    "def contains_tech_pattern(word, patterns_dict):\n",
    "    \"\"\"更严格的中文技术模式匹配\"\"\"\n",
    "    for category, patterns in patterns_dict.items():\n",
    "        for pattern in patterns:\n",
    "            # 只有当词语完全匹配或包含重要技术模式时才认为是技术词\n",
    "            if word == pattern or (len(pattern) > 1 and pattern in word):\n",
    "                return True, category\n",
    "    return False, None\n",
    "\n",
    "# 测试\n",
    "test_words = ['阿里云', '数据库设计', '消息中间件', '普通经验']\n",
    "for word in test_words:\n",
    "    is_tech, category = contains_tech_pattern(word, CHINESE_TECH_PATTERNS)\n",
    "    print(f\"'{word}': 技术词={is_tech}, 类别={category}\")\n",
    "    \n",
    "\n",
    "\n",
    "def enhanced_chinese_keyword_extraction(jd_text):\n",
    "    \"\"\"\n",
    "    增强版中文技术关键词提取\n",
    "    \"\"\"\n",
    "    # 1. 预处理：保护英文术语\n",
    "    protected_text, protected_terms = protect_english_terms(jd_text)\n",
    "    \n",
    "    # 2. spaCy处理\n",
    "    doc = nlp(protected_text)\n",
    "    \n",
    "    # 3. 恢复术语并提取\n",
    "    keywords = set()\n",
    "    stop_words = {'经验', '要求', '熟悉', '优先', '以上', '团队', '沟通', '负责'}\n",
    "    \n",
    "    for token in doc:\n",
    "        # 恢复原始术语\n",
    "        original_text = token.text\n",
    "        if token.text.startswith('__TERM_'):\n",
    "            original_text = protected_terms.get(token.text, token.text)\n",
    "        \n",
    "        # 跳过停用词和标点\n",
    "        if original_text in stop_words or token.pos_ == 'PUNCT':\n",
    "            continue\n",
    "            \n",
    "        # 策略1：实体识别\n",
    "        if token.ent_type_ in ['ORG', 'PRODUCT', 'GPE']:\n",
    "            keywords.add(original_text)\n",
    "            \n",
    "        # 策略2：英文术语（包括恢复后的）\n",
    "        elif original_text.isascii() and len(original_text) > 1:\n",
    "            keywords.add(original_text)\n",
    "            \n",
    "        # 策略3：中文技术模式匹配（你的建议！）\n",
    "        elif not original_text.isascii():  # 如果是中文\n",
    "            is_tech, category = contains_tech_pattern(original_text, CHINESE_TECH_PATTERNS)\n",
    "            if is_tech:\n",
    "                keywords.add(original_text)\n",
    "                \n",
    "        # 策略4：特定技术名词\n",
    "        elif token.pos_ == 'NOUN' and original_text in ['容器', '微服务', '分布式', '缓存']:\n",
    "            keywords.add(original_text)\n",
    "    \n",
    "    # 4. 后处理：确保常见技术词被包含\n",
    "    tech_whitelist = ['redis', 'mysql', 'python', 'java', 'docker', 'kubernetes', 'k8s', 'aws']\n",
    "    for term in tech_whitelist:\n",
    "        if term in jd_text.lower():\n",
    "            keywords.add(term)\n",
    "    \n",
    "    return list(keywords)\n",
    "\n",
    "# 测试\n",
    "test_jd = \"\"\"\n",
    "需要熟悉Redis、MySQL和K8s技术，有AWS经验。\n",
    "熟悉阿里云平台操作，了解数据库优化。\n",
    "具备微服务架构设计能力。\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "result = enhanced_chinese_keyword_extraction(test_jd)\n",
    "print(\"增强版提取结果:\", result)\n",
    "# 预期应该包含: ['Redis', 'MySQL', 'K8s', 'AWS', '阿里云', '数据库', '微服务']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "417578f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'error': {'message': 'Not found the model gpt-3.5-turbo or Permission denied', 'type': 'resource_not_found_error'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 76\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# 测试\u001b[39;00m\n\u001b[1;32m     55\u001b[0m jd_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124m职位名称：高级前端开发工程师\u001b[39m\n\u001b[1;32m     57\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124m- 与后端工程师协作完成接口联调\u001b[39m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 76\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mparse_jd_with_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjd_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mprint\u001b[39m(json\u001b[38;5;241m.\u001b[39mdumps(result, ensure_ascii\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n",
      "Cell \u001b[0;32mIn[48], line 37\u001b[0m, in \u001b[0;36mparse_jd_with_llm\u001b[0;34m(jd_text)\u001b[0m\n\u001b[1;32m     18\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124m你是一个资深的HR专家和技术招聘者。你的任务是从下面的职位描述（JD）中精确提取信息，并严格按照指定的JSON格式输出。\u001b[39m\n\u001b[1;32m     20\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;132;01m{\u001b[39;00mjd_text\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 37\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 温度设为0，确保输出确定性\u001b[39;49;00m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;66;03m# 从AI的回复中提取JSON部分\u001b[39;00m\n\u001b[1;32m     44\u001b[0m         result_text \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[0;32m~/miniconda3/envs/ml-learning/lib/python3.10/site-packages/openai/_utils/_utils.py:287\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml-learning/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:1147\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m   1103\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m   1145\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m   1146\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m-> 1147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt_cache_key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msafety_identifier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mverbosity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweb_search_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[1;32m   1188\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml-learning/lib/python3.10/site-packages/openai/_base_client.py:1259\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1246\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1247\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1254\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1255\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1256\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1257\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1258\u001b[0m     )\n\u001b[0;32m-> 1259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml-learning/lib/python3.10/site-packages/openai/_base_client.py:1047\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1044\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1046\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1047\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Error code: 404 - {'error': {'message': 'Not found the model gpt-3.5-turbo or Permission denied', 'type': 'resource_not_found_error'}}"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key = os.getenv('OPENAI_API_KEY'),\n",
    "    base_url = os.getenv('OPENAI_BASE_URL'),\n",
    ")\n",
    "model_name = os.getenv('MODEL_NAME')\n",
    "\n",
    "def parse_jd_with_llm(jd_text):\n",
    "    \"\"\"\n",
    "    使用LLM解析JD，返回结构化数据\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "你是一个资深的HR专家和技术招聘者。你的任务是从下面的职位描述（JD）中精确提取信息，并严格按照指定的JSON格式输出。\n",
    "\n",
    "# 提取要求：\n",
    "1.  job_title: 提取职位名称（如：前端开发工程师、后端开发工程师）。\n",
    "2.  skills: 提取所有提到的技术技能、编程语言、工具和框架。这是一个数组。\n",
    "3.  experience_years: 提取工作经验要求，通常是一个数字或数字范围（如\"3年\"提取为3，\"3-5年\"提取为\"3-5\"）。如果没有明确要求，则为null。\n",
    "4.  education: 提取学历要求（如\"本科及以上\"）。如果没有明确要求，则为null。\n",
    "5.  responsibilities: 提取职位核心职责，总结为2-5个要点。这是一个数组。\n",
    "6.  nice_to_have: 提取\"优先考虑\"、\"加分项\"、\"有以下经验者优先\"后面的内容。这是一个数组。\n",
    "\n",
    "# 输出格式：\n",
    "你必须严格输出一个且仅一个JSON对象，不要有任何其他解释。\n",
    "\n",
    "# 职位描述：\n",
    "{jd_text}\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0  # 温度设为0，确保输出确定性\n",
    "        )\n",
    "        \n",
    "        # 从AI的回复中提取JSON部分\n",
    "        result_text = response.choices[0].message.content\n",
    "        # 解析JSON字符串为Python字典\n",
    "        structured_data = json.loads(result_text)\n",
    "        return structured_data\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"解析JSON失败，AI返回了非JSON内容：\")\n",
    "        print(result_text)\n",
    "        return None\n",
    "\n",
    "# 测试\n",
    "jd_text = \"\"\"\n",
    "职位名称：高级前端开发工程师\n",
    "\n",
    "职位描述：\n",
    "我们正在寻找一位具有3年以上前端开发经验的高手。你需要负责核心产品的前端架构设计与开发，主要使用React和TypeScript技术栈。\n",
    "\n",
    "职位要求：\n",
    "- 精通JavaScript/TypeScript，熟练掌握React及其生态圈\n",
    "- 有3-5年大型前端项目开发经验\n",
    "- 本科及以上学历，计算机相关专业优先\n",
    "- 有性能优化经验者优先\n",
    "- 熟悉Webpack、Vite等构建工具\n",
    "- 有小程序开发经验者优先\n",
    "\n",
    "职位职责：\n",
    "- 负责公司主要产品的前端模块开发\n",
    "- 参与技术选型和架构设计\n",
    "- 优化前端性能，提升用户体验\n",
    "- 与后端工程师协作完成接口联调\n",
    "\"\"\"\n",
    "\n",
    "result = parse_jd_with_llm(jd_text)\n",
    "if result:\n",
    "    print(json.dumps(result, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919b12bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
